{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EngrAsadKhan/Assignment-PAI-/blob/main/Quiz2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SP24-RAI-008/ Muhammad Asad Khan"
      ],
      "metadata": {
        "id": "icSsdC8N-Ox7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quiz-2\n",
        "Perform following tasks on the provided Reviews Dataset.\n",
        "\n",
        "Drop words if not alphabets.\n",
        "Tokenize the sentence.\n",
        "Perform lemitization.\n",
        "Vectorize using bigram and trigram techniques.\n",
        "Apply Random Forest algorithm with 150 trees.\n",
        "Evaluate overall accuracy of the model and class-wise precision"
      ],
      "metadata": {
        "id": "FhnbQChE50ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "stopwords.words('english')\n",
        "exclude = string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5-bcmcNee4l",
        "outputId": "45d21ac6-e899-4b52-d2f3-ca863cbda34f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = pd.read_csv('/content/reviews_dataset.csv')\n",
        "df = temp_df.iloc[:30000]"
      ],
      "metadata": {
        "id": "IT9OMIfMe0jm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_non_alphabets(text):\n",
        "    \"\"\"Drops words if not alphabets.\"\"\"\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        words = text.split()\n",
        "        cleaned_words = [word for word in words if word.isalpha()]\n",
        "        return ' '.join(cleaned_words)\n",
        "\n",
        "    elif isinstance(text, list):\n",
        "        text_str = ' '.join(text)  # Join list elements into a single string\n",
        "        words = text_str.split()\n",
        "        cleaned_words = [word for word in words if word.isalpha()]\n",
        "        return ' '.join(cleaned_words)\n",
        "    else:\n",
        "        return text\n",
        "df['news'] = df['news'].apply(drop_non_alphabets)\n",
        "\n",
        "def tokenize_sentence(text):\n",
        "    \"\"\"Tokenizes the sentence.\"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \"\"\"Performs lemmatization.\"\"\"\n",
        "    # Ensure the input is a string\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)  # Convert to string if not already\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "1pKTCdM-e9ul"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['news'] = df['news'].apply(drop_non_alphabets)\n",
        "df['news'] = df['news'].astype(str).apply(tokenize_sentence)\n",
        "\n",
        "df['news'] = df['news'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "gMJGFEx7kbKF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "#print(X_train.head)\n",
        "\n",
        "#print(X_train)\n",
        "#print(X_test)\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the training data and transform it\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the same vectorizer\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Output the shapes of the resulting Bag of Words matrices\n",
        "print(f\"Shape of X_train_bow: {X_train_bow.shape}\")\n",
        "print(f\"Shape of X_test_bow: {X_test_bow.shape}\")\n",
        "\n",
        "# Applying Random Forest Classifier\n",
        "rf = RandomForestClassifier(150)\n",
        "\n",
        "rf.fit(X_train_bow,y_train)\n",
        "y_pred = rf.predict(X_test_bow)\n",
        "#accuracy_score(y_test,y_pred)\n",
        "\n",
        "print (accuracy_score(y_test,y_pred))\n",
        "print (confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu52GS7sSEbM",
        "outputId": "58764ead-8ec0-4bbb-c01b-29f247b145a7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1780,)\n",
            "Shape of X_train_bow: (1780, 30962)\n",
            "Shape of X_test_bow: (445, 30962)\n",
            "0.950561797752809\n",
            "[[109   1   3   1   1]\n",
            " [  2  66   2   1   1]\n",
            " [  3   0  73   0   0]\n",
            " [  0   0   0 102   0]\n",
            " [  5   1   0   1  73]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(ngram_range=(2,2))\n",
        "\n",
        "X_train_n_gram = cv.fit_transform(X_train)\n",
        "X_test_n_gram = cv.transform(X_test)\n",
        "\n",
        "# Output the shapes of the resulting Bag of Words matrices\n",
        "print(f\"Shape of X_train_bow: {X_train_n_gram.shape}\")\n",
        "print(f\"Shape of X_test_bow: {X_test_n_gram.shape}\")\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "rf.fit(X_train_n_gram,y_train)\n",
        "y_pred = rf.predict(X_test_n_gram)\n",
        "\n",
        "print (accuracy_score(y_test,y_pred))\n",
        "print (confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m0KGuBSNatZ",
        "outputId": "e1d33763-4dc6-4904-c057-7ca293a9e091"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_bow: (1780, 285808)\n",
            "Shape of X_test_bow: (445, 285808)\n",
            "0.8696629213483146\n",
            "[[109   1   2   3   0]\n",
            " [ 10  47   1  14   0]\n",
            " [  4   0  66   6   0]\n",
            " [  0   0   0 102   0]\n",
            " [ 14   1   0   2  63]]\n"
          ]
        }
      ]
    }
  ]
}